ğŸ“¦ Databricks ML Project: Endâ€‘toâ€‘End ML Pipeline with MLflow, Feature Engineering, and Workflows
Using TPCDS_SF1000 Retail & Eâ€‘Commerce Dataset
ğŸ“˜ Overview
This project demonstrates a full endâ€‘toâ€‘end Machine Learning workflow on Databricks, showcasing:

PySparkâ€‘based feature engineering at scale

MLflow experiment tracking

Model Registry lifecycle management

Databricks Workflows orchestration

Delta Lakeâ€“based data pipelines

Productionâ€‘ready batch inference

The dataset used is TPCDS_SF1000, a largeâ€‘scale synthetic retail + eâ€‘commerce dataset commonly used for benchmarking. It includes customer behavior, orders, web interactions, store sales, and more â€” ideal for demonstrating realâ€‘world ML engineering.

ğŸ¯ Project Goal
Predict customer spending behavior using historical store and web sales data.

This is a realistic business problem for omnichannel retailers and allows you to demonstrate:

Multiâ€‘table joins

Timeâ€‘based aggregations

Customerâ€‘level feature engineering

Regression modeling

Model deployment workflows

ğŸ—ï¸ Architecture
Code
Databricks Workspace
â”‚
â”œâ”€â”€ Data Layer (Delta Lake)
â”‚     â”œâ”€â”€ Raw TPCDS tables
â”‚     â”œâ”€â”€ Cleaned & joined feature tables
â”‚     â””â”€â”€ Predictions output table
â”‚
â”œâ”€â”€ Notebooks / Scripts
â”‚     â”œâ”€â”€ 01_data_ingestion.py
â”‚     â”œâ”€â”€ 02_feature_engineering.py
â”‚     â”œâ”€â”€ 03_model_training.py
â”‚     â”œâ”€â”€ 04_batch_inference.py
â”‚
â”œâ”€â”€ MLflow
â”‚     â”œâ”€â”€ Experiment tracking
â”‚     â”œâ”€â”€ Model artifacts
â”‚     â””â”€â”€ Model Registry (Staging â†’ Production)
â”‚
â””â”€â”€ Databricks Workflows
      â”œâ”€â”€ Ingest
      â”œâ”€â”€ Feature Engineering
      â”œâ”€â”€ Train & Register Model
      â””â”€â”€ Batch Inference
ğŸ“‚ Repository Structure
Code
/ml_project/
    â”œâ”€â”€ notebooks/
    â”‚     â”œâ”€â”€ 01_data_ingestion.py
    â”‚     â”œâ”€â”€ 02_feature_engineering.py
    â”‚     â”œâ”€â”€ 03_model_training.py
    â”‚     â”œâ”€â”€ 04_batch_inference.py
    â”‚
    â”œâ”€â”€ src/
    â”‚     â”œâ”€â”€ features.py
    â”‚     â”œâ”€â”€ train.py
    â”‚     â”œâ”€â”€ utils.py
    â”‚
    â”œâ”€â”€ conf/
    â”‚     â”œâ”€â”€ config.yaml
    â”‚
    â”œâ”€â”€ tests/
    â””â”€â”€ README.md
ğŸ§± 1. Data Ingestion
The project uses the TPCDS_SF1000 dataset available in Databricks sample data.

Example tables used:

store_sales

web_sales

customer

date_dim

item

The ingestion notebook:

Reads raw tables

Selects relevant columns

Writes cleaned Delta tables to /mnt/raw/tpcds/

ğŸ§ª 2. Feature Engineering (PySpark)
This is the heart of the project and demonstrates your Data Engineering strength.

Key transformations:

Customerâ€‘level aggregations (total spend, avg spend, recency)

Store vs. web channel behavior

Timeâ€‘based features using date_dim

Window functions for rolling metrics

Joining multiple fact + dimension tables

Writing feature tables to Delta Lake

Example engineered features:

Feature	Description
total_store_sales	Total spend in physical stores
total_web_sales	Total spend online
avg_basket_value	Mean order value
days_since_last_purchase	Recency metric
rolling_30d_spend	30â€‘day rolling spend window
Output is saved to:

Code
/mnt/features/tpcds/customer_features
ğŸ¤– 3. Model Training with MLflow
The training notebook:

Loads engineered features

Splits into train/test

Trains a regression model (Random Forest, XGBoost, or Spark ML)

Logs parameters, metrics, and artifacts to MLflow

Registers the model in the Model Registry

Example MLflow logging:

python
with mlflow.start_run():
    mlflow.log_params(model_params)
    mlflow.log_metric("rmse", rmse)
    mlflow.spark.log_model(model, "model")
ğŸ›ï¸ 4. Model Registry Workflow
The model is registered as:

Code
models:/tpcds_customer_spend_model
Lifecycle stages:

None â†’ initial registration

Staging â†’ validation

Production â†’ used for inference

You can promote/demote versions through the UI or API.

ğŸ“¦ 5. Batch Inference Pipeline
The inference notebook:

Loads the Production model

Scores new customer data

Writes predictions to Delta Lake

Output table:

Code
/mnt/predictions/tpcds/customer_spend_predictions
This simulates a real production scoring job.

ğŸ”„ 6. Databricks Workflow (Job Orchestration)
A Databricks Workflow orchestrates the entire pipeline:

Ingest Data

Feature Engineering

Train Model

Register Model

Batch Inference

Each task depends on the previous one, forming a DAG.

ğŸ§ª Testing
Unit tests cover:

Feature transformations

Utility functions

Schema validation

Tests run locally or in Databricks Repos.
